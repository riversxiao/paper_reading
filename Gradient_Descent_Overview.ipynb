{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降概览"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目录\n",
    "\n",
    "1. 梯度下降变形\n",
    "\n",
    "> 批次梯度下降(batch gradient descent)\n",
    "\n",
    "> 随机梯度下降(stochastic gradient descent)\n",
    "\n",
    "> 迷你批次梯度下降(mini-batch gradient decent)\n",
    "\n",
    "2. 挑战\n",
    "\n",
    "3. 梯度下降优化算法\n",
    "\n",
    "> 动量(momentum)\n",
    "\n",
    "> 涅斯捷罗夫的梯度法(nesterov accelerated gradient)\n",
    "\n",
    "> (Adagrad)\n",
    "\n",
    "> (Adadelta)\n",
    "\n",
    "> (RMSprop)\n",
    "\n",
    "> (Adam)\n",
    "\n",
    "> (AdaMax)\n",
    "\n",
    "> (Nadam)\n",
    "\n",
    "> 算法可视化\n",
    "\n",
    "> 该选择哪种优化算法？\n",
    "\n",
    "4. 并行式和分布式随机梯度下降\n",
    "\n",
    "> Hogwild!\n",
    "\n",
    "> Downpour SGD\n",
    "\n",
    "> Delay-tolerant Algorithms for SGD\n",
    "\n",
    "> TesorFlow\n",
    "\n",
    "> Elastic Averaging SGD\n",
    "\n",
    "5. 优化随机梯度下降的其他策略\n",
    "\n",
    "> Shuffling and Curriculum Learning\n",
    "\n",
    "> 批次标准化\n",
    "\n",
    "> 较早的停止\n",
    "\n",
    "> 噪点(Gradient Noise)\n",
    "\n",
    "6. 结尾\n",
    "\n",
    "7. 参考 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. At the same time, every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize gradient descent (e.g. lasagne's, caffe's, and keras' documentation). These algorithms, however, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前为止,在常用优化神经网络模型的算法里, 梯度下降是最流行的几种之一. 同时,当前最好的一些深度学习库里面也包含了各种...然而,这些算法通常被当作黑匣子优化, 因为目前的解释都有点牵强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This blog post aims at providing you with intuitions towards the behaviour of different algorithms for optimizing gradient descent that will help you put them to use. We are first going to look at the different variants of gradient descent. We will then briefly summarize challenges during training. Subsequently, we will introduce the most common optimization algorithms by showing their motivation to resolve these challenges and how this leads to the derivation of their update rules. We will also take a short look at algorithms and architectures to optimize gradient descent in a parallel and distributed setting. Finally, we will consider additional strategies that are helpful for optimizing gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这篇博客希望能给你提供一些直觉,了解不同梯度下降算法的特点，从而更好的使用它们。我们首先会了解梯度下降的各种变形。然后，我们将简要总结在训练过程中的遇到的挑战。随后，我们将介绍最常见的优化算法，***他们解决这些挑战的动机Z***，以及如何导致更新规则的推导。我们还将简要介绍并行和分布式情况下使用算法和架构去优化梯度下降。最后，我们将考虑有助于优化梯度下降的其他策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a way to minimize an objective function J(θ) \n",
    " parameterized by a model's parameters θ∈R d  \n",
    "θ∈Rd\n",
    " by updating the parameters in the opposite direction of the gradient of the objective function ∇ θ J(θ) \n",
    "∇θJ(θ)\n",
    " w.r.t. to the parameters. The learning rate η \n",
    "η\n",
    " determines the size of the steps we take to reach a (local) minimum. In other words, we follow the direction of the slope of the surface created by the objective function downhill until we reach a valley. If you are unfamiliar with gradient descent, you can find a good introduction on optimizing neural networks here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降的各种变形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
